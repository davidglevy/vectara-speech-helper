{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d79e3b4-ceab-4021-9721-f48a22c4a134",
   "metadata": {},
   "source": [
    "# 05 - Prompt Engineering\n",
    "We'll now exlore the possibilities with prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b5eb6-854e-4b4a-b5aa-423796cfac61",
   "metadata": {},
   "source": [
    "## Lab Setup\n",
    "We'll setup our lab and use the public reports from a few Australia public sector agencies for our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e796428c-0fa1-47d0-b5a9-67f7ba11f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q vectara-skunk-client==0.4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a3d0be2-2439-49b2-bdef-8a158710f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:48:35 +1100 lab_setup            INFO:User prefix for lab: david\n",
      "09:48:35 +1100 lab_setup            INFO:Setting up lab corpus with name [david-05-lab-prompt-engineering]\n",
      "09:48:35 +1100 Factory              INFO:initializing builder\n",
      "09:48:35 +1100 Factory              INFO:Factory will load configuration from home directory\n",
      "09:48:35 +1100 root                 INFO:We are processing authentication type [OAuth2]\n",
      "09:48:35 +1100 root                 INFO:initializing Client\n",
      "09:48:43 +1100 AdminService         INFO:Created new corpus with 257\n",
      "09:48:43 +1100 root                 INFO:New corpus created CreateCorpusResponse(corpusId=257, status=Status(code=<StatusCode.OK: 0>, statusDetail='Corpus Created', cause=None))\n",
      "09:48:43 +1100 lab_setup            INFO:New lab created with id [257]\n"
     ]
    }
   ],
   "source": [
    "from lab_setup import create_lab_corpus\n",
    "corpus_id = create_lab_corpus(\"05-lab-prompt-engineering\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e7be61-9f05-4660-b623-4c360af95036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:48:43 +1100 Factory              INFO:initializing builder\n",
      "09:48:43 +1100 Factory              INFO:Factory will load configuration from home directory\n",
      "09:48:43 +1100 root                 INFO:We are processing authentication type [OAuth2]\n",
      "09:48:43 +1100 root                 INFO:initializing Client\n",
      "09:48:43 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"257\"}\n",
      "Avoiding Hallucinations in LLMs.pdf: 3.31MB [00:05, 630kB/s]                                                         \n",
      "09:48:50 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"257\"}\n",
      "RAG done right - part 1 - chunking.pdf: 4.69MB [00:06, 757kB/s]                                                      \n",
      "09:48:56 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"257\"}\n",
      "RAG done right - part 2 - retrieval.pdf: 3.32MB [00:07, 466kB/s]                                                     \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vectara_client.core import Factory\n",
    "\n",
    "resources_dir = Path(\"./resources/05_prompt_engineering/vectara\")\n",
    "client = Factory().build()\n",
    "indexer_service = client.indexer_service\n",
    "\n",
    "for p in resources_dir.glob(\"*.pdf\"):\n",
    "    indexer_service.upload(corpus_id, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5604d9e-bf8a-47fe-84a1-695e7d1f18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:49:06 +1100 Factory              INFO:initializing builder\n",
      "09:49:06 +1100 Factory              INFO:Factory will load configuration from home directory\n",
      "09:49:06 +1100 root                 INFO:We are processing authentication type [OAuth2]\n",
      "09:49:06 +1100 root                 INFO:initializing Client\n"
     ]
    }
   ],
   "source": [
    "from vectara_client.core import Factory\n",
    "\n",
    "client = Factory().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4804f-14de-4524-a86f-02de7dfac254",
   "metadata": {},
   "source": [
    "## Summarization Using Default Prompt\n",
    "The query below uses our default summarizer (vectara-summary-ext-v1.2.0 aka GPT 3.5) to return the results using RAG only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b68b06d-dd96-4e83-aa29-248fc8f7622d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara's platform better than other LLM options?\n",
       "\n",
       "Vectara's platform stands out among other LLM options due to its focus on avoiding hallucinations in LLM-powered applications [1]. The platform utilizes Retrieval Augmented Generation (RAG) in a comprehensive and effective manner [2][3]. RAG is done right by Vectara, ensuring accurate chunking and retrieval [3]. These features contribute to the reliability and superiority of Vectara's platform compared to other LLM options.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lab_setup import render_response\n",
    "\n",
    "query = \"Why is Vectara's platform better than other LLM options?\"\n",
    "qs = client.query_service\n",
    "resp = qs.query(query, corpus_id)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df64d70-8334-433f-a5b1-38b118a63adc",
   "metadata": {},
   "source": [
    "## Summarization Using Default Prompt\n",
    "We're now creating a custom prompt with the following preferences:\n",
    "\n",
    "1. We want to tailor the style by setting `persona=\"Head of Investor Relations\"`\n",
    "2. We show a more concise answer instead of citing each result by setting `cite=False`\n",
    "3. We will also allow _general_context_ to creep in from the base LLM by setting the `just_rag=False` . Whilst this may increase the risk of hallucinations, if done right it can bolster the results from the retrieval model.\n",
    "4. We create a more concise result by setting `max_word_count=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d78ff8d-8965-480f-b078-84568d34ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:49:14 +1100 root                 INFO:Here's our tailored \"promptText\" we'll be supplying on the query:\n",
      "\n",
      "[ {\"role\": \"system\", \"content\": \"You are a Head of Investor Relations who takes the search results and only return the most relevant answer. Do not iterate over each question, preferably based on the search results in this chat. You may allow additional information you know in the results. Respond in the language denoted by ISO 639 code \\\"$vectaraLangCode\\\".\"}, \n",
      "#foreach ($qResult in $vectaraQueryResults) \n",
      "   #if ($foreach.first) \n",
      "   {\"role\": \"user\", \"content\": \"Search for \\\"$esc.java(${vectaraQuery})\\\", and give me the first search result.\"}, \n",
      "   {\"role\": \"assistant\", \"content\": \"$esc.java(${qResult.getText()})\" }, \n",
      "   #else \n",
      "   {\"role\": \"user\", \"content\": \"Give me the \\\"$vectaraIdxWord[$foreach.index]\\\" search result.\"}, \n",
      "   {\"role\": \"assistant\", \"content\": \"$esc.java(${qResult.getText()})\" }, \n",
      "   #end \n",
      " #end \n",
      "{\"role\": \"user\", \"content\": \"Generate a detailed answer (that is no more than 100 words) for the query \\\"$esc.java(${vectaraQuery})\\\" preferably based on the search results in this chat. You may allow additional information you know in the results. Do not cite search results.\" } ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara's platform better than other LLM options?\n",
       "\n",
       "Vectara's platform stands out among other LLM options due to its effective approach in avoiding hallucinations in applications powered by LLM. The use of Retrieval Augmented Generation (RAG) done right ensures accurate and reliable results. Vectara's platform also incorporates chunking, which further enhances the quality of information retrieval. With a focus on preventing hallucinations and ensuring robust retrieval and generation, Vectara offers a superior LLM solution that provides reliable and trustworthy outcomes for its users.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectara_client.util import SimplePromptFactory\n",
    "import logging\n",
    "\n",
    "prompt_factory = SimplePromptFactory(persona=\"Head of Investor Relations\", cite=False, max_word_count=100, just_rag=False)\n",
    "prompt_text = prompt_factory.build()\n",
    "\n",
    "logging.info(f\"Here's our tailored \\\"promptText\\\" we'll be supplying on the query:\\n\\n{prompt_text}\\n\")\n",
    "\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512aaca-410b-49c5-b5a6-56f2ed9f8644",
   "metadata": {},
   "source": [
    "## Persona: ELI5\n",
    "Now lets respond as though we're doing it using \"Explain Like I'm 5\" language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5202af4-d659-4681-b443-7e235d3e0b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara's platform better than other LLM options?\n",
       "\n",
       "Vectara's platform stands out among other LLM options due to its ability to prevent hallucinations in LLM-powered applications. This is achieved through a retrieval augmented generation (RAG) approach, which ensures accurate and reliable results. Vectara's platform also excels in chunking, enhancing the retrieval process. With its focus on maintaining the fidelity of generated content, Vectara offers a superior solution compared to other LLM options. Its commitment to addressing the limitations of LLM technology sets it apart, making it a preferred choice for those seeking trustworthy and dependable results.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_factory = SimplePromptFactory(persona=\"Explaning to someone in ELI5 language\", cite=False, max_word_count=100, just_rag=False)\n",
    "prompt_text = prompt_factory.build()\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a9ee7b-a529-46f3-b7e0-d620699b3607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Explain what RAG is?\n",
       "\n",
       "RAG, short for Retrieval Augmented Generation, is a concept that involves combining retrieval-based and generation-based approaches in natural language processing. It aims to improve the quality and coherence of generated text by incorporating information from retrieved sources. In the context of RAG, chunking and retrieval are essential components for achieving better results in text generation[1][2]. The use of RAG helps avoid hallucinations and enhances the reliability of LLM-powered applications[3].\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Explain what RAG is?\"\n",
    "prompt_factory = SimplePromptFactory(persona=\"Explaning to someone in ELI5 language\", cite=True, max_word_count=100, just_rag=True)\n",
    "prompt_text = prompt_factory.build()\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b527d62-22d5-4104-8f9c-c88ae716b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: What is steam power?\n",
       "\n",
       "Steam power refers to the use of steam as a source of energy to generate power. It is commonly used in various applications and industries. The first search result, \"Avoiding hallucinations in LLM-powered Applications,\" does not directly provide information about steam power. However, based on the context, steam power is not related to this result. The second and third search results, \"Retrieval Augmented Generation (RAG) Done Right_ Retrieval\" and \"Retrieval Augmented Generation (RAG) Done Right_ Chunking,\" also do not seem to be relevant to the query. Therefore, based on the search results in this chat, we couldn't find a specific answer regarding steam power.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is steam power?\"\n",
    "prompt_factory = SimplePromptFactory(persona=\"Explaning to someone in ELI5 language\", cite=True, max_word_count=100, just_rag=False)\n",
    "prompt_text = prompt_factory.build()\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
